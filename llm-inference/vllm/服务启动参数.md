


引擎启动参数：

- https://docs.vllm.ai/en/stable/models/engine_args.html

并行配置参数：

- https://github.com/vllm-project/vllm/blob/v0.5.2/vllm/config.py


max-num-seqs：默认 256，

当 max-num-seqs 比较小时，较迟接收到的 request 会进入 waiting_list，直到前面有request 结束后再被添加进生成队列。

当 max-num-seqs 太大时，会出现一部分 request 在生成了 3-4 个 tokens 之后，被加入到 waiting_list（有些用户出现生成到一半卡住的情况）。过大或过小的 max-num-seqs 都会影响用户体验。


max-num-batched-tokens：很重要的配置，比如你配置了 max-num-batched-tokens=1000 那么你大概能在一个 batch 里面处理 10 条平均长度约为 100 tokens 的 inputs。max-num-batched-tokens 应尽可能大，来充分发挥 continuous batching 的优势。不过似乎（对于 TGI 是这样，vllm 不太确定），在提供 HF 模型时，该 max-num-batched-tokens 能够被自动推导出来。



--max-model-len  模型上下文长度。


--enable-prefix-caching
Enables automatic prefix caching.






```
--served-model-name：API 中使用的模型名称。如果提供了多个名称，服务将响应任何提供的名称。
--device：vLLM 执行的设备类型。默认值：auto，可能的选择有：auto、cuda、neuron、cpu、openvino、tpu、xpu。
--model：要使用的 Huggingface 模型的名称或路径。默认值："facebook/opt-125m"。
--tokenizer：要使用的 Huggingface 分词器的名称或路径。如果未指定，将使用模型名称或路径。
--trust-remote-code：信任来自 Huggingface 的远程代码。默认值：false。
--download-dir：下载和加载权重的目录，默认为huggingface的默认缓存目录。
--load-format：要加载的模型权重的格式。可能的选择：auto、pt、safetensors、npcache、dummy、tensorizer、sharded_state、gguf、bitsandbytes、mistral。默认值：auto，将尝试以 safetensors 格式加载权重，如果 safetensors 格式不可用，则回退到 pytorch bin 格式。
--dtype：模型权重和激活的数据类型。可能的选择：auto、half、float16、bfloat16、float、float32。
--kv-cache-dtype：kv缓存存储的数据类型。可能的选择：auto、fp8、fp8_e5m2、fp8_e4m3，如果为auto，将使用模型数据类型。
--max-model-len：模型上下文长度。如果未指定，将自动从模型配置中派生。
--pipeline-parallel-size, -pp：流水线并行大小。默认值：1。
--tensor-parallel-size, -tp：张量并行大小。默认值：1。
--enable-prefix-caching：启用自动前缀缓存。默认值：false。
--gpu-memory-utilization：用于模型执行的 GPU 内存比例，范围为 0 到 1。例如：值 0.5 表示 GPU 内存利用率为 50%。如果未指定，将使用默认值 0.9。
--disable-custom-all-reduce：禁用自定义all-reduce Kernel并回退到使用NCCL。
--max-num-batched-tokens：每次迭代的最大批处理Token数。
--enable-chunked-prefill：如果设置，预填充请求可以根据 max_num_batched_tokens 进行分块。
--max-num-seqs：每次迭代的最大序列数。默认值：256。
```


```
--kv-cache-dtype：kv缓存存储的数据类型。可能的选择：auto、fp8、fp8_e5m2、fp8_e4m3，如果为auto，将使用模型数据类型。
--quantization, -q：用于量化权重的方法。可能的选择：aqlm、awq、deepspeedfp、tpu_int8、fp8、fbgemm_fp8、modelopt、marlin、gguf、gptq_marlin_24、gptq_marlin、awq_marlin、gptq、compressed-tensors、bitsandbytes、qqq、experts_int8、neuron_quant、ipex、None。如果为None，首先检查模型配置文件中的quantization_config属性；如果为 None，先假设模型权重未量化，并使用dtype来确定权重的数据类型。
```












