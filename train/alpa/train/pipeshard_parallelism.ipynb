{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Distributed Training with Both Shard and Pipeline Parallelism\n\nAlpa can automatically parallelizes jax functions with both shard\nparallelism (a.k.a. intra-operator parallelism) and pipeline parallelism\n(a.k.a. inter-operator parallelism). Shard parallelism includes\ndata parallelism, operator parallelism, and their combinations.\nThe previous `quick start <alpa-quickstart>` tutorial focuses on\nusing Alpa for shard parallelism.\n\nIn this tutorial, we show how to use Alpa with both shard and pipeline parallelism.\nFirst, we show how to use Alpa to manually assign stages for pipeline parallelism.\nThen we show how to use Alpa to automate this process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries and Initialize Environment\nFirst, import the required libraries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import alpa\nfrom alpa.testing import assert_allclose\nimport copy\nfrom flax import linen as nn\nfrom flax.training.train_state import TrainState\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nimport optax\nimport ray\n\nalpa.util.disable_tqdm_globally()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to a Ray Cluster\nAlpa uses a distributed framework `ray <https://docs.ray.io/>`_ to manage\nthe cluster and disributed workers. We initialize ray and alpa.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ray.init()\nalpa.init(cluster=\"ray\")\n\n# Alternatively, you can use the following command to connect to an existing\n# ray cluster.\n# ray.init(address=\"auto\")\n#\n# Note: `alpa.init(cluster=\"ray\")` uses the gpus resources of the whole ray\n# cluster. To configure Alpa to only use a subset of gpu resources, one can \n# specific the number of nodes and number of gpus per node.\n# For example, only run 2 gpus when 8 gpus are available \n# alpa.init('ray', devices_per_node=2, num_nodes=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train an MLP on a Single Device\nIn this tutorial, we use a toy dataset to train an MLP model.\nSpecifically, we use the model to fit the function: $y = Wx + b$.\nNote that now this model is being executed on CPU because we force the driver\nprocess to use the CPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n    hidden_dim: int\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        return x\n\n\ndim = 2048\nbatch_size = 2048\n\n# Generate ground truth W and b\nrngkey = jax.random.PRNGKey(0)\nk1, k2 = random.split(rngkey)\nW = random.normal(k1, (dim, dim))\nb = random.normal(k2, (dim,))\n\n# Generate the training data\nksample, knoise = random.split(k1)\nx = random.normal(ksample, (batch_size, dim))\ny = (x @ W + b) + 0.1 * random.normal(knoise, (batch_size, dim))\n\n# Initialize a train state, which includes the model paramter and optimizer\n# state.\nmodel = MLPModel(hidden_dim=dim)\nparams = model.init(rngkey, x)\ntx = optax.adam(learning_rate=1e-3)\nstate = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n\n\n# Define the training step\ndef train_step(state, batch):\n\n    def loss_func(params):\n        out = model.apply(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    grads = jax.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\n\nbatch = {\"x\": x, \"y\": y}\nexpected_state = train_step(state, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Parallelism with Manual Assignment\nPipeline paralleism requires partitioning the model into several pipeline\nstages. To manually assign stages, we can use ``alpa.mark_pipeline_boundary``\nto mark the boundary of each pipeline stage in the forward function.\nNote that each pipeline stage is also automatically parallelized by the\nshard parallel pass.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a MLP model with manual stage boundaries.\nclass ManualPipelineMLPModel(nn.Module):\n    hidden_dim: int\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        # Use this boundary marker to separate the model into two stages.\n        alpa.mark_pipeline_boundary()\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        return x\n\n\n# Initialize the train state with the same parameters as the single-device\n# model.\nmanual_pipeline_model = ManualPipelineMLPModel(hidden_dim=dim)\nmanual_pipeline_state = TrainState.create(apply_fn=manual_pipeline_model.apply,\n                                          params=copy.deepcopy(params),\n                                          tx=tx)\n\n\n# Define the training step.\n# We use the \"alpa.PipeshardParallel\" option to let alpa use both\n# pipeline parallelism and shard parallelism. To make pipeline parallelism\n# efficient, we need to fill the pipeline with many micro batches,\n# so a `num_micro_batches` should be specified.\n@alpa.parallelize(method=alpa.PipeshardParallel(num_micro_batches=16,\n                                                layer_option=\"manual\"))\ndef manual_pipeline_train_step(state, batch):\n\n    def loss_func(params):\n        out = state.apply_fn(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    # We use `alpa.grad` here to separate the apply gradient stage with the\n    # forward/backward stages in the pipeline. This is necessary to ensure that\n    # the gradient accumulation is correct.\n    grads = alpa.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\n\nmanual_pipeline_actual_state = manual_pipeline_train_step(\n    manual_pipeline_state, batch)\nassert_allclose(expected_state.params,\n                manual_pipeline_actual_state.params,\n                atol=5e-3)\n\nalpa.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>In addition, Alpa supports more flexible manual assignments of pipeline\n  parallelism strategies. In the above example, each partitioned stages will\n  be assigned an equal number of devices to run. If you want to control the\n  device assignment of each stage, you can use the more advanced\n  ``stage_option=alpa.ManualStageOption``.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Parallelism with Automatic Assignment\nAlpa also supports automatically partitioning the model into multiple\npipeline stages and assign each pipeline stage a device mesh such that\nthe total execution latency is minimized. Specifically, the automatic\npartitioning algorithm consists of the following steps:\n\n1. **Layer Construction:** In this step, the operators in the model are\n   clustered into \"layers\" based on a graph clustering algorithm. The\n   user needs to specify the total number of layers (i.e. clusters) as\n   a hyperparameter.\n2. **Stage Construction and Mesh Slicing:** In this step, we partition\n   the device cluster (device mesh) to multiple submeshes and assign\n   layers to submeshes to form pipeline stages to minimize the total\n   pipeline execution latency.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alpa.init(cluster=\"ray\")\n\n# Define the parallel method.\n# `alpa.AutoLayerOption(layer_num=2)` means we use the auto layer construcion\n# algorithm to cluster primitive operators into two layers.\n# `stage_option=\"auto\"` means we enable the auto stage construction algorithm.\nmethod = alpa.PipeshardParallel(num_micro_batches=16,\n                                layer_option=alpa.AutoLayerOption(layer_num=2),\n                                stage_option=\"auto\")\n\n\n# Define the training step. The function body is the same as the above one.\n@alpa.parallelize(method=method)\ndef auto_pipeline_train_step(state, batch):\n\n    def loss_func(params):\n        out = state.apply_fn(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    # Again, we use `alpa.grad` here to separate the apply gradient stage with\n    # the forward/backward stages in the pipeline.\n    grads = alpa.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\n\n# In the first call, alpa triggers the compilation.\n# The compilation first profiles several costs and solves an optimization\n# problem to get the optimal pipeline assignments.\nauto_pipeline_actual_state = auto_pipeline_train_step(state, batch)\nassert_allclose(expected_state.params,\n                auto_pipeline_actual_state.params,\n                atol=5e-3)\n\nalpa.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpret the Results\n**Some basic concepts**\n- Cluster mesh and submeshes\n    - Cluster mesh is a computer cluster that contains GPUs. A ``N\u00d7M`` cluster mesh means the cluster has ``N`` physical machines and each machine has ``M`` GPUs.\n    - Submeshes can be obtained by slicing from the cluster mesh. For example, given a ``N\u00d7M`` cluster mesh, a submesh ``(1, M)`` means using all GPUs in one physical machine.\n    - For more details on how Alpa uses submeshes to solve *inter-operator parallelism*, you can read the **Section 5: Inter-Operator Parallelism** in the `Alpa paper <https://arxiv.org/pdf/2201.12023.pdf>`_.\n- Device mesh and logical mesh\n    - A device mesh is a 2-dimensional logical view of a set of physical devices.\n    - For a set of physical devices, there can be multiple logical views. For example, given 2 nodes and 8 GPUs per node (i.e., 16 devices in total), we can view them as a 2\u00d78, 1\u00d716, 4\u00d74, 8\u00d72, or 16\u00d71 device mesh.\n    - The mapping between physical devices and the logical device mesh view is optimized by the inter-op pass\n        - Hence, you can see ``Result mesh_shapes`` and the corresponding ``Result logical_mesh_shapes`` in the optimization output.\n\nWith the basic concepts in mind, you now can better understand the ``ModuleProfileResult``:\n- ``ModuleProfileResult``: ``result[(i, j, s, c), m]`` means this stage contains forward layers ``i, i+1, ..., j`` and corresponding backward layers, and runs under the ``s``-th submesh and the ``c``-th auto sharding config for the submesh. The ``m = 0`` means the result is for the forward pass, and ``m = 1`` for backward pass.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}